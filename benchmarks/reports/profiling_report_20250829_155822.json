{
  "attention": {
    "error": "/home/kd/med-vllm/medvllm/layers/attention.py:40: UserWarning: Flash Attention is not installed. Falling back to a simpler attention implementation. For better performance, install flash_attn with: pip install flash-attn --no-build-isolation\n  warnings.warn(\n2025-08-29 15:58:27 - medvllm.engine.model_runner.registry - INFO - Registered default model: biobert-base-cased-v1.2\n2025-08-29 15:58:27 - medvllm.engine.model_runner.registry - INFO - Registered default model: clinical-bert-base-uncased\nusage: benchmark_attention.py [-h] [--seq SEQ] [--heads HEADS] [--dim DIM]\n                              [--batch BATCH] [--iters ITERS]\n                              [--device {auto,cpu,cuda}]\n                              [--dtype {fp32,fp16,bf16}]\n                              [--suite {basic,extended}] [--save [SAVE]]\n                              [--emit-trace] [--trace-dir TRACE_DIR]\n                              [--fusion-compile]\nbenchmark_attention.py: error: unrecognized arguments: --out /home/kd/med-vllm/benchmarks/benchmark_results_cpu_smoke/attn_cpu_20250829_155822.json --warmup 1"
  },
  "imaging": {
    "device": "cpu",
    "dtype": "fp32",
    "channels_last": false,
    "amp": false,
    "cudnn_benchmark": false,
    "compiled": false,
    "input_shape": [
      2,
      8,
      256,
      256
    ],
    "batches": 2,
    "batch_time_ms": 30.769142998906318,
    "imgs_per_sec": 65.0001854153393,
    "cpu_max_rss_mb": 818.2265625,
    "cuda_max_mem_mb": null,
    "acc_check_enabled": false,
    "has_nan": null,
    "has_inf": null,
    "repeatability_pass": null,
    "max_abs_diff": null,
    "mean_abs_diff": null
  },
  "meta": {
    "device": "cpu",
    "trace_dir": "/home/kd/med-vllm/benchmarks/profiles",
    "timestamp": "2025-08-29T15:58:39.541058"
  }
}